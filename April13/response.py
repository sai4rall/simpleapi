Thank you for your request to enhance the existing FastAPI program further. Based on your input, I’ll add two new endpoints to the program:
	1	/predict_on_csv: This endpoint will allow anomaly detection using previously saved CSV files (from the data/predict/{namespace}/ directory) instead of fetching fresh metrics from Prometheus. It will use the same trained Isolation Forest model and service graph to predict anomalies and identify cascading failures, respecting the two_minutes and five_minutes flags for persistence, and the last_minutes parameter to filter data.
	2	/fetch_metrics: This endpoint will fetch metrics from Prometheus for a given namespace and save them as CSV files in data/predict/{namespace}/, similar to what happens during /predict. It will not perform anomaly detection, only data collection.
I’ll incorporate these endpoints into the existing program while maintaining all prior functionality, including:
	•	Handling 100 microservices with 10 metrics (some service-specific) across namespaces.
	•	Using metrics.json and services.json for configuration with Mustache variable substitution.
	•	Treating (pod_name, job) combinations as data points with pod indices (pod_1, pod_2, etc.).
	•	Saving training and prediction data to CSVs.
	•	Supporting last_days for /train and last_minutes for /predict.
	•	Generating service and cascading failure graphs.
	•	Filtering business-time data and outliers for training.
Assumptions
	•	The /predict_on_csv endpoint uses CSVs generated by /predict or /fetch_metrics in data/predict/{namespace}/.
	•	The /fetch_metrics endpoint mirrors the data collection logic of /predict but skips anomaly detection.
	•	Both endpoints accept namespace and other relevant parameters (e.g., last_minutes for /fetch_metrics, last_minutes, two_minutes, five_minutes for /predict_on_csv).
	•	Existing restrictions (no prometheus-api-client) and data filtering (business time, outliers) remain.
Updated Directory Structure
project/
├── metrics.json
├── services.json
├── models/                   # Stores .pkl files for models and graphs
├── images/                   # Stores graph images
├── data/
│   ├── train/               # Stores training data CSVs
│   ├── predict/             # Stores prediction data CSVs
├── main.py                   # FastAPI application
Example Config Files
The metrics.json and services.json remain unchanged from the previous version. For reference:
`metrics.json`
[
    {
        "metric_name": "http_requests_total",
        "query": "rate(http_requests_total{namespace=\"{{namespace}}\", job=\"{{jobname}}\"}[5m])",
        "applicable_to": "all"
    },
    {
        "metric_name": "db_query_latency",
        "query": "histogram_quantile(0.99, sum(rate(db_query_duration_seconds_bucket{namespace=\"{{namespace}}\", job=\"{{jobname}}\", dbname=\"{{dbname}}\"}[5m])) by (le))",
        "applicable_to": ["service_1", "service_2"]
    },
    {
        "metric_name": "cpu_usage",
        "query": "sum(rate(container_cpu_usage_seconds_total{namespace=\"{{namespace}}\", job=\"{{jobname}}\"}[5m]))",
        "applicable_to": "all"
    },
    {
        "metric_name": "memory_usage",
        "query": "sum by (pod_name, job)(increase(jvm_memory_used_bytes{namespace=\"{{namespace}}\", job=\"{{jobname}}\"}[5m]))",
        "applicable_to": "all"
    },
    {
        "metric_name": "network_bytes",
        "query": "sum(rate(container_network_receive_bytes_total{namespace=\"{{namespace}}\", job=\"{{jobname}}\"}[5m]))",
        "applicable_to": "all"
    },
    {
        "metric_name": "error_rate",
        "query": "rate(http_requests_total{namespace=\"{{namespace}}\", job=\"{{jobname}}\", status=~\"5..\"}[5m])",
        "applicable_to": "all"
    },
    {
        "metric_name": "request_duration",
        "query": "rate(http_request_duration_seconds_sum{namespace=\"{{namespace}}\", job=\"{{jobname}}\"}[5m])",
        "applicable_to": "all"
    },
    {
        "metric_name": "db_connections",
        "query": "db_connections{namespace=\"{{namespace}}\", job=\"{{jobname}}\", dbname=\"{{dbname}}\"}",
        "applicable_to": ["service_1", "service_2"]
    },
    {
        "metric_name": "queue_length",
        "query": "queue_length{namespace=\"{{namespace}}\", job=\"{{jobname}}\"}",
        "applicable_to": "all"
    },
    {
        "metric_name": "incoming_traffic",
        "query": "sum(rate(http_requests_total{namespace=\"{{namespace}}\", job=\"{{jobname}}\"}[5m]))",
        "applicable_to": "all"
    }
]
`services.json`
{
    "services": [
        {
            "service_name": "service_1",
            "jobname": "webapp",
            "dbname": "mydb1"
        },
        {
            "service_name": "service_2",
            "jobname": "api",
            "dbname": "mydb2"
        },
        {
            "service_name": "service_100",
            "jobname": "worker",
            "dbname": null
        }
    ]
}
Updated Python Program (`main.py`)
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import requests
import pandas as pd
import numpy as np
import json
import logging
from datetime import datetime, timedelta
from string import Template
from sklearn.ensemble import IsolationForest
import joblib
import networkx as nx
import matplotlib.pyplot as plt
import os
from typing import Optional

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# FastAPI app
app = FastAPI()

# Configuration
PROMETHEUS_URL = "http://your-prometheus-server:9090"  # Update with your Prometheus URL
METRICS_FILE = "metrics.json"
SERVICES_FILE = "services.json"
MODEL_DIR = "models"
IMAGE_DIR = "images"
DATA_DIR = "data"
TRAIN_DATA_DIR = os.path.join(DATA_DIR, "train")
PREDICT_DATA_DIR = os.path.join(DATA_DIR, "predict")
STEP = "5m"  # Query resolution
CONTAMINATION = 0.1  # Isolation Forest contamination parameter

# Ensure directories exist
os.makedirs(MODEL_DIR, exist_ok=True)
os.makedirs(IMAGE_DIR, exist_ok=True)
os.makedirs(TRAIN_DATA_DIR, exist_ok=True)
os.makedirs(PREDICT_DATA_DIR, exist_ok=True)

def load_config(file_path):
    """Load JSON configuration file."""
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except Exception as e:
        logging.error(f"Error loading {file_path}: {e}")
        raise

def substitute_query(query, variables):
    """Substitute Mustache-like variables in the query."""
    try:
        query = query.replace('{{', '${').replace('}}', '}')
        return Template(query).substitute(**variables)
    except KeyError as e:
        logging.error(f"Missing variable in query: {e}")
        return None
    except Exception as e:
        logging.error(f"Error substituting query: {e}")
        return None

def query_prometheus(query, start_time, end_time, step=STEP):
    """Query Prometheus API directly."""
    try:
        start_ts = int(start_time.timestamp())
        end_ts = int(end_time.timestamp())
        url = f"{PROMETHEUS_URL}/api/v1/query_range"
        params = {
 disseminated to other parties without the express permission of xAI.
            "query": query,
            "start": start_ts,
            "end": end_ts,
            "step": step
        }
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()
        if data['status'] != 'success' or not data['data']['result']:
            logging.warning(f"No data for query: {query}")
            return None
        return data['data']['result']
    except Exception as e:
        logging.error(f"Error querying Prometheus: {e}")
        return None

def process_prometheus_data(result, metric_name):
    """Convert Prometheus query result to DataFrame, handling pod_name and job."""
    if not result:
        return pd.DataFrame()
    
    data = []
    pod_indices = {}
    next_pod_index = 1
    
    for res in result:
        metric = res['metric']
        pod_name = metric.get('pod_name', 'unknown')
        job = metric.get('job', 'unknown')
        
        pod_key = (pod_name, job)
        if pod_key not in pod_indices:
            pod_indices[pod_key] = f"pod_{next_pod_index}"
            next_pod_index += 1
        
        pod_index = pod_indices[pod_key]
        
        for timestamp, value in res['values']:
            try:
                data.append({
                    'timestamp': pd.to_datetime(timestamp, unit='s'),
                    'pod_index': pod_index,
                    'job': job,
                    'value': float(value)
                })
            except ValueError:
                continue
    
    df = pd.DataFrame(data)
    if df.empty:
        return df
    
    if 'pod_index' in df.columns:
        df_pivot = df.pivot(index='timestamp', columns='pod_index', values='value')
        df_pivot.columns = [f"{metric_name}_{col}" for col in df_pivot.columns]
        df_pivot.reset_index(inplace=True)
        return df_pivot
    return df

def filter_outliers(df, columns):
    """Remove top/bottom 10% outliers for specified columns."""
    if df.empty:
        return df
    df_filtered = df.copy()
    for col in columns:
        if col in df.columns:
            lower_bound = df[col].quantile(0.1)
            upper_bound = df[col].quantile(0.9)
            df_filtered = df_filtered[
                (df_filtered[col].isna()) | 
                ((df_filtered[col] >= lower_bound) & (df_filtered[col] <= upper_bound))
            ]
    return df_filtered

def generate_service_graph(dataframes, services):
    """Generate a service dependency graph with weighted edges based on correlations."""
    G = nx.DiGraph()
    for service in services:
        G.add_node(service)
    
    for i, s1 in enumerate(services):
        for s2 in services[i+1:]:
            if s1 in dataframes and s2 in dataframes:
                for metric in dataframes[s1]:
                    if metric in dataframes[s2]:
                        try:
                            df1 = dataframes[s1][metric]
                            df2 = dataframes[s2][metric]
                            if not df1.empty and not df2.empty:
                                merged = df1.merge(df2, on='timestamp', suffixes=('_1', '_2'))
                                for col1 in [c for c in merged.columns if c.endswith('_1')]:
                                    for col2 in [c for c in merged.columns if c.endswith('_2')]:
                                        corr = merged[col1].corr(merged[col2])
                                        if not np.isnan(corr) and abs(corr) > 0.5:
                                            G.add_edge(s1, s2, weight=abs(corr))
                                            G.add_edge(s2, s1, weight=abs(corr))
                        except Exception as e:
                            logging.warning(f"Error computing correlation between {s1} and {s2}: {e}")
    
    return G

def save_graph_image(G, filename, title):
    """Save network graph as an image."""
    plt.figure(figsize=(12, 8))
    pos = nx.spring_layout(G)
    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=500, font_size=8)
    edge_labels = nx.get_edge_attributes(G, 'weight')
    nx.draw_networkx_edge_labels(G, pos, edge_labels={k: f"{v:.2f}" for k, v in edge_labels.items()})
    plt.title(title)
    plt.savefig(filename, format='png')
    plt.close()

class TrainRequest(BaseModel):
    namespace: str
    last_days: Optional[int] = 7

class PredictRequest(BaseModel):
    namespace: str
    last_minutes: Optional[int] = 60
    two_minutes: Optional[bool] = False
    five_minutes: Optional[bool] = False

class FetchMetricsRequest(BaseModel):
    namespace: str
    last_minutes: Optional[int] = 60

@app.post("/train")
async def train_model(request: TrainRequest):
    """Train Isolation Forest model for a namespace."""
    namespace = request.namespace
    last_days = request.last_days
    
    metrics_config = load_config(METRICS_FILE)
    services_config = load_config(SERVICES_FILE)
    
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(days=last_days)
    
    dataframes = {}
    services = [s['service_name'] for s in services_config['services']]
    
    incoming_traffic = {}
    
    for service_config in services_config['services']:
        service = service_config['service_name']
        dataframes[service] = {}
        variables = {
            'namespace': namespace,
            'jobname': service_config.get('jobname', ''),
            'dbname': service_config.get('dbname', '') or ''
        }
        
        for metric in metrics_config:
            metric_name = metric['metric_name']
            query_template = metric['query']
            applicable_to = metric['applicable_to']
            
            if applicable_to != 'all' and service not in applicable_to:
                continue
            
            query = substitute_query(query_template, variables)
            if not query:
                continue
            
            result = query_prometheus(query, start_time, end_time)
            df = process_prometheus_data(result, metric_name)
            
            if metric_name == 'incoming_traffic':
                incoming_traffic[service] = df
            else:
                df = filter_outliers(df, [c for c in df.columns if c != 'timestamp'])
                dataframes[service][metric_name] = df
    
    os.makedirs(os.path.join(TRAIN_DATA_DIR, namespace), exist_ok=True)
    for service in dataframes:
        for metric, df in dataframes[service].items():
            if not df.empty:
                csv_path = os.path.join(TRAIN_DATA_DIR, namespace, f"{service}_{metric}.csv")
                df.to_csv(csv_path, index=False)
                logging.info(f"Saved training data to {csv_path}")
    
    filtered_data = []
    feature_columns = [m['metric_name'] for m in metrics_config if m['metric_name'] != 'incoming_traffic']
    
    for service in services:
        if service not in incoming_traffic or incoming_traffic[service].empty:
            continue
        
        traffic_df = incoming_traffic[service]
        traffic_cols = [c for c in traffic_df.columns if c.startswith('incoming_traffic_')]
        if not traffic_cols:
            continue
        
        has_traffic = False
        for col in traffic_cols:
            if (traffic_df[col] > 0).any():
                has_traffic = True
                break
        if not has_traffic:
            continue
        
        service_data = []
        for metric in feature_columns:
            if metric in dataframes[service] and not dataframes[service][metric].empty:
                df = dataframes[service][metric]
                traffic_timestamps = traffic_df[traffic_df[traffic_cols].gt(0).any(axis=1)]['timestamp']
                df = df[df['timestamp'].isin(traffic_timestamps)]
                if not df.empty:
                    for col in [c for c in df.columns if c != 'timestamp']:
                        service_data.append(df[col].values)
        
        if service_data:
            max_len = max(len(d) for d in service_data)
            service_data = [np.pad(d, (0, max_len - len(d)), constant_values=np.nan) for d in service_data]
            service_data = np.array(service_data).T
            filtered_data.append(service_data)
    
    if not filtered_data:
        raise HTTPException(status_code=400, detail="No valid data for training")
    
    combined_data = np.vstack(filtered_data)
    combined_data = np.nan_to_num(combined_data, nan=0.0)
    
    model = IsolationForest(contamination=CONTAMINATION, random_state=42)
    model.fit(combined_data)
    
    model_path = os.path.join(MODEL_DIR, f"{namespace}_model.pkl")
    joblib.dump(model, model_path)
    
    G = generate_service_graph(dataframes, services)
    graph_path = os.path.join(MODEL_DIR, f"{namespace}_graph.pkl")
    joblib.dump(G, graph_path)
    
    image_path = os.path.join(IMAGE_DIR, f"{namespace}_service_graph.png")
    save_graph_image(G, image_path, f"Service Graph for {namespace}")
    
    return {"status": "success", "namespace": namespace, "model_path": model_path, "graph_image": image_path}

@app.post("/fetch_metrics")
async def fetch_metrics(request: FetchMetricsRequest):
    """Fetch metrics from Prometheus and save to CSV."""
    namespace = request.namespace
    last_minutes = request.last_minutes
    
    metrics_config = load_config(METRICS_FILE)
    services_config = load_config(SERVICES_FILE)
    
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(minutes=last_minutes)
    
    dataframes = {}
    services = [s['service_name'] for s in services_config['services']]
    
    for service_config in services_config['services']:
        service = service_config['service_name']
        dataframes[service] = {}
        variables = {
            'namespace': namespace,
            'jobname': service_config.get('jobname', ''),
            'dbname': service_config.get('dbname', '') or ''
        }
        
        for metric in metrics_config:
            metric_name = metric['metric_name']
            query_template = metric['query']
            applicable_to = metric['applicable_to']
            
            if applicable_to != 'all' and service not in applicable_to:
                continue
            
            query = substitute_query(query_template, variables)
            if not query:
                continue
            
            result = query_prometheus(query, start_time, end_time)
            df = process_prometheus_data(result, metric_name)
            dataframes[service][metric_name] = df
    
    os.makedirs(os.path.join(PREDICT_DATA_DIR, namespace), exist_ok=True)
    csv_files = []
    for service in dataframes:
        for metric, df in dataframes[service].items():
            if not df.empty:
                csv_path = os.path.join(PREDICT_DATA_DIR, namespace, f"{service}_{metric}.csv")
                df.to_csv(csv_path, index=False)
                logging.info(f"Saved metrics data to {csv_path}")
                csv_files.append(csv_path)
    
    if not csv_files:
        raise HTTPException(status_code=400, detail="No metrics data fetched")
    
    return {"status": "success", "namespace": namespace, "csv_files": csv_files}

@app.post("/predict")
async def predict_anomalies(request: PredictRequest):
    """Predict anomalies using fresh Prometheus data."""
    namespace = request.namespace
    last_minutes = request.last_minutes
    two_minutes = request.two_minutes
    five_minutes = request.five_minutes
    
    model_path = os.path.join(MODEL_DIR, f"{namespace}_model.pkl")
    graph_path = os.path.join(MODEL_DIR, f"{namespace}_graph.pkl")
    
    if not os.path.exists(model_path) or not os.path.exists(graph_path):
        raise HTTPException(status_code=404, detail=f"Model or graph not found for {namespace}")
    
    model = joblib.load(model_path)
    G = joblib.load(graph_path)
    
    metrics_config = load_config(METRICS_FILE)
    services_config = load_config(SERVICES_FILE)
    
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(minutes=last_minutes)
    
    dataframes = {}
    services = [s['service_name'] for s in services_config['services']]
    
    for service_config in services_config['services']:
        service = service_config['service_name']
        dataframes[service] = {}
        variables = {
            'namespace': namespace,
            'jobname': service_config.get('jobname', ''),
            'dbname': service_config.get('dbname', '') or ''
        }
        
        for metric in metrics_config:
            metric_name = metric['metric_name']
            query_template = metric['query']
            applicable_to = metric['applicable_to']
            
            if applicable_to != 'all' and service not in applicable_to:
                continue
            
            query = substitute_query(query_template, variables)
            if not query:
                continue
            
            result = query_prometheus(query, start_time, end_time)
            df = process_prometheus_data(result, metric_name)
            dataframes[service][metric_name] = df
    
    os.makedirs(os.path.join(PREDICT_DATA_DIR, namespace), exist_ok=True)
    for service in dataframes:
        for metric, df in dataframes[service].items():
            if not df.empty:
                csv_path = os.path.join(PREDICT_DATA_DIR, namespace, f"{service}_{metric}.csv")
                df.to_csv(csv_path, index=False)
                logging.info(f"Saved prediction data to {csv_path}")
    
    return predict_common(namespace, dataframes, services, model, G, two_minutes, five_minutes)

@app.post("/predict_on_csv")
async def predict_on_csv(request: PredictRequest):
    """Predict anomalies using saved CSV files."""
    namespace = request.namespace
    last_minutes = request.last_minutes
    two_minutes = request.two_minutes
    five_minutes = request.five_minutes
    
    model_path = os.path.join(MODEL_DIR, f"{namespace}_model.pkl")
    graph_path = os.path.join(MODEL_DIR, f"{namespace}_graph.pkl")
    
    if not os.path.exists(model_path) or not os.path.exists(graph_path):
        raise HTTPException(status_code=404, detail=f"Model or graph not found for {namespace}")
    
    model = joblib.load(model_path)
    G = joblib.load(graph_path)
    
    services_config = load_config(SERVICES_FILE)
    metrics_config = load_config(METRICS_FILE)
    
    services = [s['service_name'] for s in services_config['services']]
    dataframes = {}
    
    csv_dir = os.path.join(PREDICT_DATA_DIR, namespace)
    if not os.path.exists(csv_dir):
        raise HTTPException(status_code=404, detail=f"No CSV data found for namespace {namespace}")
    
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(minutes=last_minutes)
    
    for service in services:
        dataframes[service] = {}
        for metric in metrics_config:
            metric_name = metric['metric_name']
            csv_path = os.path.join(csv_dir, f"{service}_{metric_name}.csv")
            if os.path.exists(csv_path):
                try:
                    df = pd.read_csv(csv_path)
                    df['timestamp'] = pd.to_datetime(df['timestamp'])
                    df = df[(df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)]
                    if not df.empty:
                        dataframes[service][metric_name] = df
                except Exception as e:
                    logging.warning(f"Error reading CSV {csv_path}: {e}")
    
    if not any(dataframes[s] for s in dataframes):
        raise HTTPException(status_code=400, detail="No valid CSV data found for prediction")
    
    return predict_common(namespace, dataframes, services, model, G, two_minutes, five_minutes)

def predict_common(namespace, dataframes, services, model, G, two_minutes, five_minutes):
    """Common prediction logic for /predict and /predict_on_csv."""
    feature_columns = [
        m['metric_name'] for m in load_config(METRICS_FILE)
        if m['metric_name'] != 'incoming_traffic'
    ]
    prediction_data = []
    timestamps = {}
    pod_indices = {}
    
    for service_idx, service in enumerate(services):
        service_data = []
        for metric in feature_columns:
            if metric in dataframes[service] and not dataframes[service][metric].empty:
                df = dataframes[service][metric]
                for col in [c for c in df.columns if c != 'timestamp']:
                    service_data.append(df[col].values)
                    if service not in timestamps:
                        timestamps[service] = df['timestamp'].values
                    if col.startswith(f"{metric}_pod_"):
                        pod_index = col.split('_pod_')[-1]
                        pod_indices[(service, len(service_data)-1)] = pod_index
        
        if service_data:
            max_len = max(len(d) for d in service_data)
            service_data = [np.pad(d, (0, max_len - len(d)), constant_values=0.0) for d in service_data]
            service_data = np.array(service_data).T
            prediction_data.append(service_data)
    
    if not prediction_data:
        raise HTTPException(status_code=400, detail="No data for prediction")
    
    combined_data = np.vstack(prediction_data)
    combined_data = np.nan_to_num(combined_data, nan=0.0)
    
    predictions = model.predict(combined_data)
    anomaly_indices = np.where(predictions == -1)[0]
    
    anomaly_details = []
    
    timestamps_per_service = len(timestamps[services[0]])
    for idx in anomaly_indices:
        service_idx = idx // timestamps_per_service
        time_idx = idx % timestamps_per_service
        service = services[service_idx]
        timestamp = timestamps[service][time_idx]
        pod_index = pod_indices.get((service, (idx % (len(feature_columns) * timestamps_per_service)) // timestamps_per_service), 'unknown')
        
        persistent = True
        if two_minutes or five_minutes:
            duration = timedelta(minutes=5 if five_minutes else 2)
            end_check = timestamp + duration
            check_data = combined_data[service_idx * timestamps_per_service:(service_idx + 1) * timestamps_per_service]
            check_preds = model.predict(check_data)
            check_timestamps = timestamps[service]
            persistent = False
            for i, ts in enumerate(check_timestamps):
                if ts >= timestamp and ts <= end_check and check_preds[i] == -1:
                    persistent = True
                    break
        
        if persistent:
            anomaly_details.append({
                'service': service,
                'pod_index': pod_index,
                'timestamp': str(timestamp),
                'values': combined_data[idx].tolist()
            })
    
    cascading_failures = []
    if anomaly_details:
        root_anomaly = min(anomaly_details, key=lambda x: x['timestamp'])
        root_service = root_anomaly['service']
        cascading_failures.append(root_anomaly)
        
        visited = set()
        queue = [root_service]
        while queue:
            current = queue.pop(0)
            if current in visited:
                continue
            visited.add(current)
            for neighbor in G.successors(current):
                if neighbor not in visited:
                    neighbor_anomalies = [
                        a for a in anomaly_details
                        if a['service'] == neighbor and pd.to_datetime(a['timestamp']) > pd.to_datetime(root_anomaly['timestamp'])
                    ]
                    cascading_failures.extend(neighbor_anomalies)
                    queue.append(neighbor)
    
    failure_G = nx.DiGraph()
    for anomaly in cascading_failures:
        node_label = f"{anomaly['service']}_{anomaly['pod_index']}"
        failure_G.add_node(node_label, timestamp=anomaly['timestamp'])
    for i, anomaly in enumerate(cascading_failures[:-1]):
        next_anomaly = cascading_failures[i + 1]
        failure_G.add_edge(
            f"{anomaly['service']}_{anomaly['pod_index']}",
            f"{next_anomaly['service']}_{next_anomaly['pod_index']}"
        )
    
    failure_image_path = os.path.join(IMAGE_DIR, f"{namespace}_cascading_failure.png")
    save_graph_image(failure_G, failure_image_path, f"Cascading Failure Graph for {namespace}")
    
    return {
        "namespace": namespace,
        "anomalies": anomaly_details,
        "cascading_failures": cascading_failures,
        "cascading_failure_graph": failure_image_path
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
Enhancements Explained
	1	/fetch_metrics Endpoint:
	◦	Purpose: Fetches metrics from Prometheus for a given namespace and saves them as CSV files in data/predict/{namespace}/.
	◦	Parameters:
	▪	namespace: The target namespace.
	▪	last_minutes: How many minutes of data to fetch (default: 60).
	◦	Functionality:
	▪	Queries Prometheus using the same logic as /predict (respects metrics.json and services.json).
	▪	Processes pod-level metrics with indices (pod_1, pod_2, etc.).
	▪	Saves CSVs in data/predict/{namespace}/{service}_{metric}.csv.
	▪	Returns a list of generated CSV paths.
	◦	Example Request: curl -X POST "http://localhost:8000/fetch_metrics" -H "Content-Type: application/json" -d '{"namespace": "namespace_1", "last_minutes": 30}'
	◦	 Response: {
	◦	    "status": "success",
	◦	    "namespace": "namespace_1",
	◦	    "csv_files": [
	◦	        "data/predict/namespace_1/service_1_http_requests_total.csv",
	◦	        "data/predict/namespace_1/service_1_memory_usage.csv",
	◦	        ...
	◦	    ]
	◦	}
	◦	
	2	/predict_on_csv Endpoint:
	◦	Purpose: Performs anomaly detection using CSV files stored in data/predict/{namespace}/ instead of querying Prometheus.
	◦	Parameters:
	▪	namespace: The target namespace.
	▪	last_minutes: Filters CSV data to the last N minutes (default: 60).
	▪	two_minutes: If true, anomalies must persist for 2 minutes.
	▪	five_minutes: If true, anomalies must persist for 5 minutes.
	◦	Functionality:
	▪	Loads CSVs for each service and metric.
	▪	Filters data by last_minutes to ensure recent data.
	▪	Uses the trained Isolation Forest model and service graph from models/.
	▪	Detects anomalies and identifies cascading failures, matching /predict logic.
	▪	Generates a cascading failure graph.
	▪	Returns anomalies, cascading failures, and graph path.
	◦	Example Request: curl -X POST "http://localhost:8000/predict_on_csv" -H "Content-Type: application/json" -d '{"namespace": "namespace_1", "last_minutes": 30, "two_minutes": true, "five_minutes": false}'
	◦	 Response: {
	◦	    "namespace": "namespace_1",
	◦	    "anomalies": [
	◦	        {"service": "service_1", "pod_index": "pod_1", "timestamp": "2025-04-13T10:00:00", "values": [150.0, 0.99, ...]},
	◦	        ...
	◦	    ],
	◦	    "cascading_failures": [
	◦	        {"service": "service_1", "pod_index": "pod_1", "timestamp": "2025-04-13T10:00:00", "values": [150.0, 0.99, ...]},
	◦	        ...
	◦	    ],
	◦	    "cascading_failure_graph": "images/namespace_1_cascading_failure.png"
	◦	}
	◦	
	3	Shared Prediction Logic:
	◦	Extracted common anomaly detection and cascading failure logic into predict_common to avoid code duplication between /predict and /predict_on_csv.
	◦	Ensures consistency in handling pod indices, persistence flags, and graph generation.
How to Run
	1	Prepare Config Files:
	◦	Ensure metrics.json and services.json are set up as shown.
	◦	Verify 10 metrics, including pod-level metrics like memory_usage.
	2	Set Up Directories:
	◦	Confirm models/, images/, data/train/, and data/predict/ exist.
	3	Update Prometheus URL:
	◦	Set PROMETHEUS_URL to your Prometheus server.
	4	Run the FastAPI Server: python main.py
	5	
	6	Access New Endpoints:
	◦	Fetch Metrics: curl -X POST "http://localhost:8000/fetch_metrics" -H "Content-Type: application/json" -d '{"namespace": "namespace_1", "last_minutes": 30}'
	◦	
	◦	Predict on CSV: curl -X POST "http://localhost:8000/predict_on_csv" -H "Content-Type: application/json" -d '{"namespace": "namespace_1", "last_minutes": 30, "two_minutes": true, "five_minutes": false}'
	◦	
Notes
	•	CSV Dependency: /predict_on_csv requires CSVs generated by /fetch_metrics or /predict. Ensure CSVs exist in data/predict/{namespace}/.
	•	Performance: Loading many CSVs in /predict_on_csv may be slower than querying Prometheus directly; optimize by limiting last_minutes or archiving old CSVs.
	•	Pod Indexing: Consistent across all endpoints, ensuring pod_1, pod_2, etc., are used for pod-level metrics.
	•	Error Handling: Both endpoints handle missing data, invalid CSVs, or missing models with appropriate HTTP errors.
	•	Scalability: For high pod churn or large datasets, monitor CSV storage and consider compression or cleanup scripts.
If you need additional features, clarifications, or tweaks, please let me know!
